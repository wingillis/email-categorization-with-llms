{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from toolz import groupby, valmap, keyfilter, concat\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(f\"../structured_output\")\n",
    "files = sorted(folder.glob(\"*structured_output_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path: Path):\n",
    "    df = pl.read_parquet(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k.stem: load(k) for k in files}\n",
    "grouped_data = groupby(len, data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valmap(len, grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dfs = pl.concat(concat(grouped_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = concat_dfs['model'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = \"primary_category\"\n",
    "aligned_categories = concat_dfs.pivot(\n",
    "    index=[\"date\", \"body\", \"from\", \"subject\"], on=\"model\", values=cat_col, aggregate_function=\"first\"\n",
    ").with_columns(\n",
    "    all_equal=pl.concat_list(models).list.n_unique() == 1\n",
    ").drop_nulls(subset=models)\n",
    "aligned_categories['all_equal'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove examples from prior supervised datasets (don't need to relabel for this experiment)\n",
    "for _file in Path(\"..\").glob(\"training_dataset_with_labels*.parquet\"):\n",
    "    tmp_df = pl.read_parquet(_file).drop_nulls(subset=[\"supervised_label\"])\n",
    "    aligned_categories = aligned_categories.join(tmp_df, on=[\"date\", \"body\", \"from\"], how=\"anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample of emails where all models agree\n",
    "# should have the same number of emails for each category\n",
    "filtered = aligned_categories.filter(pl.col(\"all_equal\"))\n",
    "filtered.group_by(models[0]).agg(pl.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_sample = pl.concat(\n",
    "    df.sample(n=min(10, len(df)), seed=1)\n",
    "    for df in filtered.partition_by(models[0])\n",
    ")\n",
    "len(agree_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample of emails where the models (almost) maximally disagree\n",
    "unequal_filtered = aligned_categories.filter(~pl.col(\"all_equal\"))\n",
    "\n",
    "entropies = []\n",
    "for i, _data in enumerate(unequal_filtered[models].iter_rows()):\n",
    "    vcs = pl.Series(\"categories\", _data).value_counts()\n",
    "    entropies.append(entropy(vcs[\"count\"]))\n",
    "    if i == 0:\n",
    "        print(vcs)\n",
    "        print(entropies[0])\n",
    "\n",
    "unequal_filtered = unequal_filtered.with_columns(pl.Series(\"entropy\", entropies)).sort(\n",
    "    \"entropy\", descending=True\n",
    ")\n",
    "low_entropy_sample = unequal_filtered[-75:].drop(\"entropy\")\n",
    "disagree_sample = unequal_filtered[:150].drop(\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample of emails in between the two extremes\n",
    "mid = len(unequal_filtered) // 2\n",
    "middle_sample = unequal_filtered[mid - 75:mid + 75].drop(\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_entropy_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = concat_dfs['primary_category'].value_counts(sort=True).filter(pl.col(\"primary_category\") != \"N/A\")\n",
    "val_counts.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = concat_dfs['primary_category'].unique().to_list()\n",
    "\n",
    "samples = []\n",
    "for cat in categories:\n",
    "    _tmp_df = aligned_categories.with_columns(\n",
    "        pl.fold(pl.lit(False), function=lambda acc, x: acc | x, exprs=[pl.col(m) == cat for m in models]).alias(\"has_category\")\n",
    "    ).filter(pl.col(\"has_category\"))\n",
    "    samples.append(_tmp_df.sample(n=min(50, len(_tmp_df)), seed=1))\n",
    "samples = pl.concat(samples).drop(\"has_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the three samples and save them to a parquet file\n",
    "pl.concat(\n",
    "    [agree_sample, low_entropy_sample, disagree_sample, middle_sample, samples]\n",
    ").unique(keep=\"first\", subset=[\"date\", \"body\", \"from\", \"subject\"]).write_parquet(\n",
    "    \"../training_dataset.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
