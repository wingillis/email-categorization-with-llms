{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plot_theme as pt\n",
    "\n",
    "pio.templates.default = \"plotly_white+cc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../merged_metrics_consistency.parquet\").with_columns(\n",
    "    pl.col(\"Param. count (B)\").round(2)\n",
    ")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(\"f1\", descending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"Inference duration (s)\",\n",
    "    y=\"f1\",\n",
    "    color=\"model\",\n",
    "    log_x=True,\n",
    "    labels={\"model\": \"LLM\", \"f1\": \"F1 score (weighted)\"},\n",
    "\n",
    ")\n",
    "pt.save(fig, \"inference_speed_vs_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"Param. count (B)\",\n",
    "    y=\"f1\",\n",
    "    color=\"model\",\n",
    "    log_x=True,\n",
    "    labels={\"model\": \"LLM\", \"f1\": \"F1 score (weighted)\"},\n",
    ")\n",
    "pt.save(fig, \"param_count_vs_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df.with_columns(pl.col(\"accuracy\") * 100),\n",
    "    x=\"Param. count (B)\",\n",
    "    y=\"accuracy\",\n",
    "    color=\"model\",\n",
    "    log_x=True,\n",
    "    labels={\"model\": \"LLM\", \"accuracy\": \"Accuracy (%)\"},\n",
    ")\n",
    "pt.save(fig, \"param_count_vs_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df.drop_nulls(subset=[\"entropy\"]),\n",
    "    x=\"Param. count (B)\",\n",
    "    y=\"entropy\",\n",
    "    color=\"model\",\n",
    "    log_x=True,\n",
    "    labels={\"model\": \"LLM\", \"entropy\": \"Entropy (nats)\"},\n",
    ")\n",
    "pt.save(fig, \"param_count_vs_entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    df.drop_nulls(subset=[\"entropy\"]),\n",
    "    x=\"Inference duration (s)\",\n",
    "    y=\"entropy\",\n",
    "    z=\"f1\",\n",
    "    log_x=True,\n",
    "    color=\"model\",\n",
    "    labels={\n",
    "        \"model\": \"LLM\",\n",
    "        \"f1\": \"F1 score\",\n",
    "        \"entropy\": \"Entropy\",\n",
    "        \"Inference duration (s)\": \"Inference dur.\",\n",
    "    },\n",
    "    height=550,\n",
    ")\n",
    "\n",
    "fig = fig.update_layout(\n",
    "    scene=dict(\n",
    "        annotations=[\n",
    "            dict(\n",
    "                showarrow=False,\n",
    "                x=np.log10(0.3),\n",
    "                y=0.3,\n",
    "                z=0.6,\n",
    "                text=\"Optimal<br>zone\",\n",
    "                xanchor=\"center\",\n",
    "                font=dict(color=\"mediumseagreen\", weight=\"bold\"),\n",
    "                bgcolor=\"rgba(0.4, 0.6, 0.4, 0.1)\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "fig = fig.update_scenes(\n",
    "    camera_projection_type=\"orthographic\",\n",
    ")\n",
    "pt.save(fig, \"entropy_vs_f1_vs_inference_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(col: pl.Expr) -> pl.Expr:\n",
    "    return (col - col.min()) / (col.max() - col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.drop_nulls(subset=[\"entropy\"])\n",
    "\n",
    "subset = subset.with_columns(\n",
    "    minmax(pl.col(\"Inference duration (s)\")),\n",
    "    minmax(pl.col(\"entropy\")),\n",
    "    (1 - minmax(pl.col(\"f1\"))).alias(\"f1\"),\n",
    "    (1 - minmax(pl.col(\"accuracy\"))).alias(\"accuracy\"),\n",
    ")\n",
    "\n",
    "# compute weighted average of the normalized metrics\n",
    "subset = subset.with_columns(\n",
    "    (\n",
    "        pl.col(\"Inference duration (s)\") * 0.3\n",
    "        + pl.col(\"entropy\") * 0.2\n",
    "        + pl.col(\"f1\") * 0.35\n",
    "        + pl.col(\"accuracy\") * 0.15\n",
    "    ).alias(\"Weighted avg.\")\n",
    ")\n",
    "\n",
    "subset.select([\"model\", \"Weighted avg.\"]).sort(\"Weighted avg.\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
